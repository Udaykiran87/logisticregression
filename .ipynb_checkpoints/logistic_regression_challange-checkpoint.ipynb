{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75e5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder \n",
    "from sklearn.linear_model  import Ridge,Lasso,RidgeCV, LassoCV, ElasticNet, ElasticNetCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe70dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logisticregression(object):\n",
    "    def __init__(self,dir_path):\n",
    "        self.dir_path = dir_path\n",
    "        logging.basicConfig(filename='logistic_regression.log', level=logging.DEBUG,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "        logging.info('Logisticregression class object is created.')\n",
    "        \n",
    "    def prepare_datset(self):\n",
    "        \"\"\"\n",
    "        Create a final csv-'merge.csv'from the directory folder to be used as dataframe for later stage.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None        \n",
    "        \"\"\"   \n",
    "        logging.info('Dataset preparation started from the raw data.')\n",
    "        try:\n",
    "            # assign directory\n",
    "            directory = self.dir_path\n",
    "\n",
    "            # iterate over files in\n",
    "            # that directory\n",
    "            folder_file_dict = {}\n",
    "            for filename in os.listdir(directory):\n",
    "                f = os.path.join(directory, filename)\n",
    "                # checking if it is not a file\n",
    "                if not os.path.isfile(f):\n",
    "                    file_list = [os.path.join(f, sub_filename) for sub_filename in os.listdir(f)]\n",
    "                    folder_file_dict[filename] = file_list\n",
    "\n",
    "            for key in folder_file_dict:\n",
    "                for file in folder_file_dict[key]:\n",
    "                    if file == r\"AReM\\bending2\\dataset4.csv\":\n",
    "                        with open(file, \"r\", encoding=\"shift_jis\", errors=\"\", newline=\"\" ) as f:\n",
    "                            lst = csv.reader(f)\n",
    "                            new_rows_list = []\n",
    "                            for row in lst:\n",
    "                                new_row = row[0].replace(\" \",\",\")\n",
    "                                print(new_row)\n",
    "                                new_rows_list.append(new_row)\n",
    "                        with open(file, \"w\", encoding=\"shift_jis\", errors=\"\", newline=\"\" ) as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            for row in new_rows_list:\n",
    "                                columns = [c.strip() for c in row.strip(', ').split(',')]\n",
    "                                writer.writerow(columns)                \n",
    "                        break\n",
    "\n",
    "            header = []\n",
    "            df_list = []\n",
    "            for key in folder_file_dict:\n",
    "                for file in folder_file_dict[key]:\n",
    "                    with open(file, \"r\", encoding=\"shift_jis\", errors=\"\", newline=\"\" ) as f:\n",
    "                        lst = csv.reader(f, delimiter=\",\")\n",
    "                        for i,row in enumerate(lst):\n",
    "                            if i==4:\n",
    "                                if header == []:\n",
    "                                    temp_col = row[0].replace(\"# Columns: \",\"\")\n",
    "                                    row[0] = temp_col\n",
    "                                    header = row\n",
    "                                break\n",
    "                        df = pd.DataFrame(lst)\n",
    "                        new_row = pd.DataFrame(header)\n",
    "                        df = pd.concat([new_row.T, df])\n",
    "                        new_file = file.replace('.csv','_new.csv')\n",
    "                        new_header = df.iloc[0] #grab the first row for the header\n",
    "                        df = df[1:] #take the data less the header row\n",
    "                        df.columns = new_header #set the header row as the df header\n",
    "                        df['lable']=key\n",
    "            #             df.to_csv(new_file, sep=\",\", header = True,index=False)\n",
    "                        df_list.append(df)\n",
    "            merged = pd.concat(df_list)\n",
    "            merged.to_csv('merged.csv', index=None, header=True) \n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while creating datasets from the raw data.\".format(str(e)))            \n",
    "        \n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        Load csv file as pandas dataframe.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None        \n",
    "        \"\"\"\n",
    "        logging.info('Dataset is getting loaded as pandas dataframe.')\n",
    "        try:        \n",
    "            self.df = pd.read_csv(\"merged.csv\") \n",
    "            self.df.drop(['time','Unnamed: 8'], axis=1, inplace=True)\n",
    "        except FileNotFoundError:\n",
    "            logging.error(\"File not found: exception occured while loading csv as pandas dataframe.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            logging.error(\"No data: exception occured while loading csv as pandas dataframe.\")\n",
    "        except pd.errors.ParserError:\n",
    "            logging.errornt(\"Parse error: exception occured while loading csv as pandas dataframe.\")\n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while loading csv as pandas dataframe.\".format(str(e)))\n",
    "            \n",
    "    def labelencode_y(self):\n",
    "        \"\"\"\n",
    "        Perform label encoding on target categorical column.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None        \n",
    "        \"\"\"\n",
    "        logging.info('Label encoding started for categorical features.')\n",
    "        try:\n",
    "            le = LabelEncoder()\n",
    "            y = self.df['lable']\n",
    "            labels_id = le.fit_transform(y)\n",
    "            self.df['lable'] = labels_id \n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while label encoding categorical column.\".format(str(e)))            \n",
    "        \n",
    "    def create_profile_report(self,inp_df):\n",
    "        \"\"\"\n",
    "        Create pandas profile report for the input data frame.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inp_df: Input data frame.\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None        \n",
    "        \"\"\"    \n",
    "        logging.info('Profile reporting started for dataframe.')\n",
    "        return ProfileReport(inp_df)\n",
    "        \n",
    "    def handle_outlier(self):\n",
    "        \"\"\"\n",
    "        remove outliers for the impacted feature columns.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None        \n",
    "        \"\"\"\n",
    "        logging.info('Outliers are getting removed.')\n",
    "        q = self.df['var_rss12'].quantile(.99)\n",
    "        self.df_new = self.df[self.df['var_rss12'] < q]\n",
    "\n",
    "        q = self.df_new['avg_rss13'].quantile(.99)\n",
    "        self.df_new = self.df_new[self.df_new['avg_rss13'] < q]   \n",
    "\n",
    "        q = self.df_new['var_rss13'].quantile(.95)\n",
    "        self.df_new = self.df_new[self.df_new['var_rss13'] < q]\n",
    "\n",
    "        q = self.df_new['avg_rss23'].quantile(.95)\n",
    "        self.df_new = self.df_new[self.df_new['avg_rss23'] < q]\n",
    "\n",
    "        q = self.df_new['var_rss23'].quantile(.95)\n",
    "        self.df_new = self.df_new[self.df_new['var_rss23'] < q]\n",
    "\n",
    "        q = self.df_new['avg_rss12'].quantile(.60)\n",
    "        self.df_new = self.df_new[self.df_new['avg_rss12'] < q] \n",
    "        \n",
    "    def standard_scaling(self):\n",
    "        \"\"\"\n",
    "        Perform standard scaling on input dataframe.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None        \n",
    "        \"\"\"      \n",
    "        logging.info('Standard scalling started for feature columsn.')\n",
    "        self.y = self.df_new['lable']\n",
    "        self.x = self.df_new.drop(columns=['lable'])\n",
    "        scalar = StandardScaler()\n",
    "        self.x_scaled = scalar.fit_transform(self.x)\n",
    "        self.df_new_scalar = pd.DataFrame(scalar.fit_transform(self.df_new))  \n",
    "        \n",
    "    def vif_score(self):\n",
    "        \"\"\"\n",
    "        Calculate vif score for input feature columns.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        Dataframe cotaining vif scores for the feature colums.        \n",
    "        \"\"\"    \n",
    "        logging.info('Vif score is calculation is in progress.')\n",
    "        try:\n",
    "            arr = self.x_scaled\n",
    "            return pd.DataFrame([[self.x.columns[i], variance_inflation_factor(arr,i)] for i in range(arr.shape[1])], columns=[\"FEATURE\", \"VIF_SCORE\"])\n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while vif score calculation.\".format(str(e))) \n",
    "    \n",
    "    def drop_multicolinearity_by_vif(self, vif_thresh):\n",
    "        \"\"\"\n",
    "        This functions drops tyhose columns whose values are more than threshold VIF passed as parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vif_thresh: This is the threshold VIF value above which dataset column will be dropped.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        None. \n",
    "        \"\"\"\n",
    "        logging.info('All features with VIF more than {} will be dropped from the dataset.'.format(vif_thresh))\n",
    "        try:\n",
    "            X = self.x_scaled\n",
    "            variables = [X.columns[i] for i in range(X.shape[1])]\n",
    "            dropped=True\n",
    "            while dropped:\n",
    "                dropped=False\n",
    "                vif = Parallel(n_jobs=-1,verbose=5)(delayed(variance_inflation_factor)(X[variables].values, ix) for ix in range(len(variables)))\n",
    "\n",
    "                maxloc = vif.index(max(vif))\n",
    "                if max(vif) > vif_thresh:\n",
    "                    if X[variables].columns[maxloc] is not self.predicted_col:\n",
    "                        logging.info(time.ctime() + ' dropping \\'' + X[variables].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "                        variables.pop(maxloc)\n",
    "                        dropped=True\n",
    "\n",
    "            logging.info('Remaining variables:')\n",
    "            logging.info([variables])\n",
    "            self.final_df = X[[i for i in variables]]\n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while droping some of the feature from dataset based on vif threshold.\".format(str(e)))\n",
    "            \n",
    "    \n",
    "    def train_test_split(self, test_size, random_state):\n",
    "        \"\"\"\n",
    "        Split data frame into train and test.\n",
    "         \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        Train and test data for feature and predicted columns.        \n",
    "        \"\"\"\n",
    "        logging.info('train and test split for dataframe started.')\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x_scaled , self.y , test_size = test_size , random_state = random_state)\n",
    "        \n",
    "    # get a list of models to evaluate\n",
    "    def get_models(self):\n",
    "        \"\"\"\n",
    "        Create logistic regression models instance for various hyper parametrs like penalty, regularization and solver.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        Dictionary contains all logistics regression model instances.        \n",
    "        \"\"\"\n",
    "        logging.info('various logitic regression model instances are getting created based on hyper parameters.')\n",
    "        try:\n",
    "            models = dict()\n",
    "            p = [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "            regularization = ['l1', 'l2', 'elasticnet']\n",
    "            solver = [\"lbfgs\",\"newton-cg\",\"sag\",\"saga\"]\n",
    "            l1_ratio = 0.5  # L1 weight in the Elastic-Net regularization    \n",
    "\n",
    "            for i in p:\n",
    "                for j in regularization:\n",
    "                    for k in solver:\n",
    "                        # create name for model\n",
    "                        key = str(i) + '_' + j + '_' + k\n",
    "                        # turn off penalty in some cases\n",
    "                        if i == 0.0:\n",
    "                            key = str(i) + '_' + 'none' + '_' + k\n",
    "                            # no penalty in this case\n",
    "                            models[key] = LogisticRegression(multi_class='multinomial', solver=k, penalty='none')\n",
    "                        elif k == \"lbfgs\" or \"newton-cg\":\n",
    "                            if j == \"l2\":\n",
    "                                models[key] = LogisticRegression(multi_class='multinomial', solver=k, penalty=j, C=i)\n",
    "                        elif j == 'elasticnet':\n",
    "                            models[key] = LogisticRegression(multi_class='multinomial', solver=k, penalty=j, C=i, l1_ratio=l1_ratio, tol=0.01)\n",
    "                        else:\n",
    "                            models[key] = LogisticRegression(multi_class='multinomial', solver=k, penalty=j, C=i)                \n",
    "\n",
    "            return models   \n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while creating logistics regression model instances\".format(str(e)))\n",
    "        \n",
    "    \n",
    "    def fit_evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Fit all the models with train and test data and then evaluate with various parameters.        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        Multiple dictionaries model evaluation scores.        \n",
    "        \"\"\"\n",
    "        logging.info('Fitting all the models with train and test data and then evaluate with various parameters are in progress.')        \n",
    "        try:\n",
    "            averages = ['macro', 'weighted']  # 'micro' is not allowed for multiclass problem\n",
    "            # get the models to evaluate\n",
    "            self.models = self.get_models()\n",
    "            # evaluate the models and store results\n",
    "            results, names = list(), list()\n",
    "            roc_auc_score_dict = dict()\n",
    "            precision_score_dict = dict()\n",
    "            recall_score_dict = dict()\n",
    "            recall_score_dict = dict()\n",
    "            accuracy_score_dict = dict()\n",
    "            f1_score_dict = dict()\n",
    "\n",
    "            for name, model in models.items():  \n",
    "                model.fit(self.x_train,self.y_train)\n",
    "                y_pred = model.predict(self.x_test)\n",
    "                y_pred_proba = model.predict_proba(self.x_test)\n",
    "\n",
    "                accuracy_score_dict[name] = accuracy_score(self.y_test, y_pred)\n",
    "\n",
    "                for j in averages:        \n",
    "                    roc_auc_score_dict[name+j] = roc_auc_score(self.y_test,y_pred_proba,multi_class = 'ovr', average=j)\n",
    "                    precision_score_dict[name+j] = precision_score(self.y_test, y_pred, average=j)\n",
    "                    recall_score_dict[name+j] = recall_score(self.y_test, y_pred, average=j)\n",
    "                    f1_score_dict[name+j] = f1_score(self.y_test, y_pred, average=j)\n",
    "\n",
    "            return accuracy_score_dict, roc_auc_score_dict, precision_score_dict, recall_score_dict, f1_score_dict  \n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while fitting and evaluating models\".format(str(e)))        \n",
    "           \n",
    "    # evaluate a give model using cross-validation\n",
    "    def evaluate_model_using_cv(self, model, X, y):\n",
    "        \"\"\"\n",
    "        Evaluate model using cross validation technique.        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        Returns the cross validation score for model.        \n",
    "        \"\"\" \n",
    "        logging.info('Evaluating model using cross validation started.') \n",
    "        try:\n",
    "            # define the evaluation procedure\n",
    "            cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "            # evaluate the model\n",
    "            scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "            return scores\n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while evaluating model using cross validation\".format(str(e)))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f350a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_regr = Logisticregression('AReM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1c86e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#,Task:,bending2\n",
      "#,Frequency,(Hz):,20\n",
      "#,Clock,(millisecond):,250\n",
      "#,Duration,(seconds):,120\n",
      "#,Columns:,time\n",
      "0,32.50,0.50,0.00,0.00,19.00,1.00,\n",
      "250,32.50,0.50,0.00,0.00,18.50,0.50,\n",
      "500,32.75,0.43,1.00,0.00,18.00,0.00,\n",
      "750,32.50,0.50,0.00,0.00,17.50,0.50,\n",
      "1000,32.50,0.50,7.50,0.50,17.50,0.87,\n",
      "1250,32.67,0.47,11.00,1.00,16.75,0.83,\n",
      "1500,32.50,0.50,6.25,0.83,18.00,0.00,\n",
      "1750,32.50,0.50,3.50,0.87,18.00,0.00,\n",
      "2000,32.33,0.47,6.00,0.00,18.33,0.47,\n",
      "2250,32.67,0.47,8.00,0.82,18.00,0.00,\n",
      "2500,32.50,0.50,8.33,3.30,16.67,0.47,\n",
      "2750,32.50,0.50,10.33,1.25,16.00,1.22,\n",
      "3000,32.25,0.83,9.00,3.08,16.00,0.71,\n",
      "3250,32.00,0.00,2.67,0.47,15.75,0.83,\n",
      "3500,31.75,0.43,2.75,1.09,16.50,1.12,\n",
      "3750,32.00,0.00,1.00,0.00,17.50,0.87,\n",
      "4000,32.25,0.83,0.00,0.00,17.50,0.50,\n",
      "4250,32.25,0.43,0.00,0.00,17.00,1.00,\n",
      "4500,31.75,0.43,5.33,3.30,17.75,0.43,\n",
      "4750,31.50,0.50,9.50,0.50,17.75,0.43,\n",
      "5000,31.50,0.50,5.00,2.16,17.33,0.47,\n",
      "5250,31.50,0.50,4.50,0.50,17.50,0.50,\n",
      "5500,31.67,0.47,4.50,1.12,18.00,0.00,\n",
      "5750,31.50,0.50,1.67,0.47,18.00,0.00,\n",
      "6000,31.67,0.47,1.00,0.00,18.33,0.47,\n",
      "6250,31.33,0.47,2.50,0.50,18.00,0.00,\n",
      "6500,31.50,0.50,2.67,0.47,18.00,0.00,\n",
      "6750,31.25,0.83,3.00,0.00,18.25,0.43,\n",
      "7000,31.25,0.43,2.50,0.50,18.50,0.50,\n",
      "7250,31.50,0.50,4.00,0.00,18.50,0.50,\n",
      "7500,31.50,0.50,5.00,1.22,18.50,0.50,\n",
      "7750,31.50,0.50,5.00,0.00,18.00,0.00,\n",
      "8000,31.67,0.47,8.75,0.43,17.67,0.47,\n",
      "8250,31.25,0.83,8.50,0.50,18.25,0.43,\n",
      "8500,31.25,0.83,11.00,0.71,18.00,0.00,\n",
      "8750,31.33,0.47,4.50,2.29,18.00,0.00,\n",
      "9000,31.50,0.50,2.00,0.00,18.00,0.00,\n",
      "9250,31.50,0.50,5.33,1.70,17.50,0.50,\n",
      "9500,30.75,0.43,11.00,1.22,18.00,0.00,\n",
      "9750,30.75,0.43,11.00,1.22,18.25,0.43,\n",
      "10000,30.67,0.47,5.33,1.25,18.50,0.50,\n",
      "10250,31.50,0.50,6.00,0.00,18.67,0.47,\n",
      "10500,30.75,0.43,5.67,0.47,18.50,0.50,\n",
      "10750,30.50,0.50,3.00,0.00,19.00,0.82,\n",
      "11000,30.67,0.47,6.50,1.50,18.00,0.00,\n",
      "11250,30.75,0.83,9.25,1.09,18.00,0.00,\n",
      "11500,30.67,0.47,11.00,0.82,18.00,0.00,\n",
      "11750,30.75,0.43,11.00,0.00,18.50,0.50,\n",
      "12000,30.50,0.50,11.25,0.43,18.50,0.50,\n",
      "12250,30.67,0.47,11.25,0.43,19.00,0.82,\n",
      "12500,30.50,0.50,11.00,0.71,19.75,0.83,\n",
      "12750,30.50,0.50,11.33,0.47,19.67,0.94,\n",
      "13000,31.50,0.50,9.00,1.00,19.25,1.30,\n",
      "13250,33.00,0.00,3.67,1.70,17.00,3.74,\n",
      "13500,32.67,0.47,7.33,3.09,11.00,2.92,\n",
      "13750,32.25,0.43,6.25,3.34,14.00,6.40,\n",
      "14000,33.00,0.00,8.00,3.16,23.00,0.71,\n",
      "14250,32.50,0.87,9.25,2.68,23.00,1.22,\n",
      "14500,31.50,0.50,10.75,0.83,23.25,0.83,\n",
      "14750,32.50,0.50,10.00,0.71,22.00,0.71,\n",
      "15000,32.25,0.83,11.50,0.50,21.50,0.50,\n",
      "15250,31.75,0.43,11.50,0.87,22.00,1.00,\n",
      "15500,32.25,0.43,9.75,0.43,22.25,0.83,\n",
      "15750,32.50,0.50,11.67,0.47,22.33,0.47,\n",
      "16000,32.25,0.83,11.75,0.43,22.25,0.83,\n",
      "16250,32.50,0.50,11.00,0.82,22.33,0.94,\n",
      "16500,32.25,0.83,11.75,0.43,22.00,0.71,\n",
      "16750,31.75,0.43,10.25,0.43,22.50,0.50,\n",
      "17000,31.33,0.47,10.00,0.00,22.00,0.82,\n",
      "17250,31.50,0.50,10.25,0.83,22.00,0.71,\n",
      "17500,31.33,0.47,11.33,0.94,21.75,0.83,\n",
      "17750,31.33,0.47,12.33,0.47,21.00,0.00,\n",
      "18000,31.50,0.50,13.50,0.50,19.00,0.71,\n",
      "18250,31.50,0.50,13.50,0.50,19.00,1.22,\n",
      "18500,31.33,0.47,12.33,0.47,21.75,0.83,\n",
      "18750,31.67,0.47,14.33,0.47,22.50,0.50,\n",
      "19000,31.20,0.75,13.40,0.49,22.00,0.82,\n",
      "19250,31.25,0.83,14.33,0.47,21.67,0.94,\n",
      "19500,31.67,0.47,14.50,0.50,23.00,0.82,\n",
      "19750,30.67,0.94,13.67,0.47,22.00,0.71,\n",
      "20000,31.00,0.71,13.75,0.43,21.50,0.50,\n",
      "20250,31.25,0.43,13.75,0.43,21.75,0.83,\n",
      "20500,31.50,0.50,13.50,0.50,22.00,0.71,\n",
      "20750,31.25,0.83,12.75,0.83,22.50,1.12,\n",
      "21000,31.00,0.71,13.50,0.50,21.50,2.18,\n",
      "21250,31.00,0.71,12.25,0.43,15.00,0.00,\n",
      "21500,31.75,0.43,9.00,1.63,16.33,0.94,\n",
      "21750,32.33,0.47,8.25,4.44,20.33,1.70,\n",
      "22000,31.67,0.47,12.67,0.94,16.00,1.22,\n",
      "22250,32.67,0.47,12.50,0.87,17.00,0.82,\n",
      "22500,31.25,0.83,13.75,1.09,17.00,2.00,\n",
      "22750,31.00,0.82,10.00,1.00,20.00,1.41,\n",
      "23000,30.50,0.50,9.00,0.00,20.00,0.82,\n",
      "23250,31.00,1.00,9.67,0.47,20.67,0.47,\n",
      "23500,32.00,0.00,11.67,0.47,19.50,0.50,\n",
      "23750,31.25,0.83,9.75,0.43,18.75,0.83,\n",
      "24000,31.00,1.00,10.00,0.00,17.67,0.47,\n",
      "24250,31.25,0.83,11.25,0.43,17.75,0.43,\n",
      "24500,30.00,0.00,10.50,0.50,17.67,0.47,\n",
      "24750,31.33,0.94,10.50,0.50,17.67,0.47,\n",
      "25000,31.50,0.50,10.50,0.50,18.00,0.00,\n",
      "25250,31.50,0.50,10.00,0.71,18.00,0.00,\n",
      "25500,30.50,0.50,8.50,1.12,17.50,0.50,\n",
      "25750,30.50,0.50,5.75,0.43,18.25,1.48,\n",
      "26000,31.00,0.00,8.00,0.71,17.33,0.94,\n",
      "26250,31.00,0.71,7.00,0.82,17.67,0.47,\n",
      "26500,31.50,0.50,6.33,0.47,18.33,0.47,\n",
      "26750,32.00,0.00,7.00,0.82,18.00,0.00,\n",
      "27000,31.50,0.50,8.67,0.47,18.00,0.00,\n",
      "27250,31.00,1.00,6.50,0.50,19.33,1.25,\n",
      "27500,31.00,0.82,6.00,1.41,18.75,0.83,\n",
      "27750,30.75,0.43,4.33,2.36,18.50,0.87,\n",
      "28000,30.67,0.47,2.50,0.50,17.75,0.83,\n",
      "28250,31.00,0.00,5.33,2.05,17.25,0.83,\n",
      "28500,30.67,0.94,7.00,0.82,17.75,1.79,\n",
      "28750,30.33,0.47,11.33,0.94,19.50,1.12,\n",
      "29000,30.33,0.47,12.75,0.83,18.33,1.70,\n",
      "29250,30.00,0.00,14.00,0.82,18.25,0.43,\n",
      "29500,30.00,0.00,12.00,0.00,17.33,0.47,\n",
      "29750,30.00,0.00,12.00,0.00,15.00,0.82,\n",
      "30000,30.00,0.00,11.50,0.50,15.67,0.94,\n",
      "30250,30.00,0.00,12.00,0.00,17.25,0.83,\n",
      "30500,30.00,0.00,10.75,0.83,18.75,0.83,\n",
      "30750,30.00,0.00,11.33,0.47,20.00,1.00,\n",
      "31000,30.00,0.00,9.25,1.48,19.75,0.83,\n",
      "31250,30.00,0.00,6.00,0.71,19.00,0.71,\n",
      "31500,30.00,0.00,5.50,0.50,19.75,1.30,\n",
      "31750,30.00,0.00,5.50,0.50,19.75,1.30,\n",
      "32000,30.00,0.00,10.82,1.11,19.64,0.98,\n",
      "32250,30.00,0.00,10.82,1.11,19.64,0.98,\n",
      "32500,30.00,0.00,9.67,0.47,19.33,0.94,\n",
      "32750,30.00,0.00,9.33,0.47,19.25,0.83,\n",
      "33000,30.00,0.00,9.33,0.47,18.67,0.94,\n",
      "33250,30.00,0.00,9.75,0.83,18.75,0.83,\n",
      "33500,30.00,0.00,11.75,0.43,18.50,0.50,\n",
      "33750,30.00,0.00,10.25,0.83,19.00,0.00,\n",
      "34000,30.00,0.00,10.00,0.82,19.00,0.82,\n",
      "34250,30.00,0.00,10.67,0.47,18.75,0.83,\n",
      "34500,30.00,0.00,9.00,0.00,19.50,1.12,\n",
      "34750,30.25,0.43,9.00,0.00,20.50,0.50,\n",
      "35000,30.00,0.00,9.25,3.27,18.00,1.63,\n",
      "35250,30.67,0.47,5.00,2.74,17.33,0.94,\n",
      "35500,30.00,0.00,4.75,1.30,17.33,0.47,\n",
      "35750,30.00,0.00,7.67,0.47,16.67,0.94,\n",
      "36000,30.00,0.00,8.75,0.43,17.00,0.00,\n",
      "36250,30.67,0.47,6.75,3.56,15.67,0.94,\n",
      "36500,30.33,0.47,4.00,2.83,14.50,1.50,\n",
      "36750,30.00,0.00,11.00,1.22,12.50,0.50,\n",
      "37000,30.00,0.00,12.00,0.00,13.25,1.30,\n",
      "37250,30.00,0.00,7.67,3.30,16.00,1.87,\n",
      "37500,30.25,0.43,2.00,1.22,18.25,0.43,\n",
      "37750,30.00,0.00,3.75,0.43,18.00,0.00,\n",
      "38000,30.00,0.00,2.50,0.50,18.00,0.00,\n",
      "38250,30.00,0.00,2.25,1.09,17.67,0.47,\n",
      "38500,30.00,0.00,1.25,0.43,17.67,0.47,\n",
      "38750,30.00,0.00,2.33,0.47,17.33,0.47,\n",
      "39000,30.00,0.00,4.00,0.00,17.67,0.47,\n",
      "39250,30.00,0.00,3.00,0.00,18.00,0.00,\n",
      "39500,30.00,0.00,1.80,0.98,18.00,0.00,\n",
      "39750,30.00,0.00,1.00,0.00,17.50,0.50,\n",
      "40000,30.00,0.00,2.33,0.47,17.67,0.47,\n",
      "40250,30.00,0.00,2.50,0.50,17.50,0.50,\n",
      "40500,30.00,0.00,2.00,0.71,17.50,0.50,\n",
      "40750,30.00,0.00,2.00,0.82,17.25,0.43,\n",
      "41000,30.00,0.00,4.00,0.00,16.75,1.30,\n",
      "41250,30.00,0.00,4.00,0.00,16.75,0.83,\n",
      "41500,30.00,0.00,3.00,0.00,17.33,0.47,\n",
      "41750,30.00,0.00,4.50,0.50,16.80,0.75,\n",
      "42000,30.00,0.00,2.75,0.43,17.25,0.83,\n",
      "42250,30.00,0.00,1.25,0.43,17.00,1.00,\n",
      "42500,30.00,0.00,2.00,0.00,16.50,0.50,\n",
      "42750,30.00,0.00,2.50,0.50,16.00,1.00,\n",
      "43000,30.00,0.00,2.00,0.00,16.33,0.47,\n",
      "43250,30.00,0.00,2.67,0.47,16.67,0.94,\n",
      "43500,30.00,0.00,2.50,0.50,16.00,0.82,\n",
      "43750,30.00,0.00,3.00,1.22,16.00,0.71,\n",
      "44000,30.00,0.00,2.00,0.00,16.75,0.83,\n",
      "44250,30.00,0.00,3.00,1.58,16.75,1.09,\n",
      "44500,30.00,0.00,4.50,0.87,16.25,0.83,\n",
      "44750,30.00,0.00,6.00,0.00,16.00,0.82,\n",
      "45000,30.00,0.00,6.50,0.50,15.67,0.94,\n",
      "45250,30.00,0.00,7.00,0.00,16.00,1.00,\n",
      "45500,30.00,0.00,7.33,0.47,15.50,0.50,\n",
      "45750,30.00,0.00,7.00,0.00,16.00,0.82,\n",
      "46000,30.00,0.00,7.00,0.00,15.50,0.50,\n",
      "46250,30.00,0.00,7.60,0.49,14.75,0.43,\n",
      "46500,30.00,0.00,7.33,0.47,15.67,0.47,\n",
      "46750,30.00,0.00,7.33,0.47,15.00,0.71,\n",
      "47000,30.00,0.00,7.33,0.47,15.50,0.50,\n",
      "47250,30.00,0.00,7.33,0.47,15.50,0.50,\n",
      "47500,30.00,0.00,6.50,0.50,15.67,0.47,\n",
      "47750,30.00,0.00,6.33,0.47,15.50,0.50,\n",
      "48000,30.00,0.00,6.50,0.50,15.50,0.50,\n",
      "48250,30.00,0.00,6.33,0.47,15.33,0.47,\n",
      "48500,30.00,0.00,5.50,0.50,16.00,1.00,\n",
      "48750,30.00,0.00,5.50,0.50,16.33,0.94,\n",
      "49000,30.00,0.00,6.00,0.00,15.67,0.47,\n",
      "49250,30.00,0.00,5.33,0.47,15.50,0.50,\n",
      "49500,30.00,0.00,5.33,0.47,15.50,0.50,\n",
      "49750,30.00,0.00,5.67,0.94,16.00,1.00,\n",
      "50000,30.00,0.00,5.50,0.87,16.00,1.00,\n",
      "50250,30.00,0.00,7.00,0.82,16.50,0.50,\n",
      "50500,30.00,0.00,7.00,0.00,15.67,0.47,\n",
      "50750,30.00,0.00,6.50,0.50,16.00,1.00,\n",
      "51000,30.00,0.00,6.25,0.43,15.75,0.83,\n",
      "51250,30.00,0.00,3.00,0.00,15.50,1.12,\n",
      "51500,30.00,0.00,5.67,0.47,16.33,1.25,\n",
      "51750,30.00,0.00,8.00,0.82,17.33,1.70,\n",
      "52000,30.00,0.00,6.50,2.06,17.75,0.83,\n",
      "52250,30.00,0.00,5.50,1.50,18.33,0.47,\n",
      "52500,30.00,0.00,4.33,1.89,18.25,0.43,\n",
      "52750,30.50,0.50,4.50,1.50,17.75,0.43,\n",
      "53000,30.00,0.00,3.50,1.12,19.00,0.82,\n",
      "53250,29.50,0.50,4.25,1.48,18.00,0.00,\n",
      "53500,30.00,0.00,5.00,0.00,18.00,0.00,\n",
      "53750,30.00,0.00,3.25,0.43,18.00,0.00,\n",
      "54000,30.00,0.00,3.67,0.47,17.50,0.50,\n",
      "54250,30.00,0.00,4.00,0.00,17.00,0.82,\n",
      "54500,30.00,0.00,4.50,0.50,17.67,0.47,\n",
      "54750,30.00,0.00,4.00,0.00,16.50,0.50,\n",
      "55000,30.00,0.00,6.00,0.00,16.25,0.83,\n",
      "55250,29.67,0.47,6.00,0.00,16.33,0.94,\n",
      "55500,29.75,0.43,4.50,1.12,16.75,0.83,\n",
      "55750,29.75,0.43,2.25,0.43,17.25,0.83,\n",
      "56000,30.00,0.00,5.00,1.22,16.67,0.47,\n",
      "56250,29.75,0.43,5.75,0.43,16.00,1.00,\n",
      "56500,29.75,0.43,5.75,0.43,15.75,0.83,\n",
      "56750,30.00,0.00,6.00,0.00,16.00,0.71,\n",
      "57000,29.75,0.43,5.50,0.50,16.00,1.00,\n",
      "57250,29.50,0.50,5.25,0.43,16.00,1.00,\n",
      "57500,29.25,0.43,4.25,0.43,16.00,1.00,\n",
      "57750,29.25,0.43,5.50,0.50,15.50,0.50,\n",
      "58000,29.50,0.50,6.00,0.71,15.50,0.50,\n",
      "58250,29.50,0.50,6.33,0.47,15.33,0.47,\n",
      "58500,29.50,0.50,6.00,0.00,15.50,0.50,\n",
      "58750,29.67,0.47,6.75,0.43,16.00,0.82,\n",
      "59000,29.25,0.43,6.50,0.50,16.50,1.12,\n",
      "59250,29.00,0.00,6.25,0.83,16.50,0.50,\n",
      "59500,29.00,0.00,6.00,0.00,16.25,0.83,\n",
      "59750,29.25,0.43,5.50,0.50,16.50,0.50,\n",
      "60000,29.25,0.43,5.00,0.00,17.00,1.00,\n",
      "60250,29.50,0.50,5.00,0.71,16.50,0.50,\n",
      "60500,29.50,0.50,6.00,0.00,16.50,0.50,\n",
      "60750,29.00,0.00,6.25,0.43,16.00,1.00,\n",
      "61000,29.25,0.43,6.25,0.43,15.75,0.83,\n",
      "61250,29.00,0.00,6.00,0.00,15.50,0.50,\n",
      "61500,29.00,0.00,6.00,0.00,15.75,0.83,\n",
      "61750,29.00,0.00,6.00,0.00,16.33,1.25,\n",
      "62000,29.25,0.43,6.00,0.00,16.33,0.47,\n",
      "62250,29.00,0.00,5.50,0.50,16.50,0.50,\n",
      "62500,29.00,0.00,5.75,0.43,16.75,0.83,\n",
      "62750,29.00,0.00,5.25,0.43,17.33,0.94,\n",
      "63000,29.00,0.00,5.50,0.50,17.33,0.94,\n",
      "63250,29.00,0.00,6.00,0.00,16.50,1.12,\n",
      "63500,29.00,0.00,5.00,0.00,16.25,0.83,\n",
      "63750,29.25,0.43,5.00,0.71,16.50,0.50,\n",
      "64000,29.00,0.00,5.00,0.71,17.00,0.71,\n",
      "64250,29.25,0.43,6.00,0.00,16.50,0.50,\n",
      "64500,29.00,0.00,6.25,0.43,16.25,0.83,\n",
      "64750,29.00,0.00,6.25,0.43,16.25,0.83,\n",
      "65000,29.00,0.00,4.75,0.43,16.50,1.12,\n",
      "65250,28.75,0.43,5.25,0.43,17.00,1.00,\n",
      "65500,28.67,0.47,4.67,0.47,16.50,0.50,\n",
      "65750,29.00,0.00,4.75,0.43,17.33,0.94,\n",
      "66000,29.00,0.00,5.00,0.00,17.50,0.50,\n",
      "66250,29.00,0.00,5.50,0.50,16.50,0.50,\n",
      "66500,29.00,0.00,5.00,0.00,16.00,1.00,\n",
      "66750,29.00,0.00,5.75,0.43,15.67,0.47,\n",
      "67000,29.00,0.00,6.75,0.43,15.50,0.50,\n",
      "67250,29.00,0.00,7.00,0.00,15.67,0.94,\n",
      "67500,28.75,0.43,6.00,0.00,16.50,1.12,\n",
      "67750,28.75,0.43,4.75,0.43,17.00,1.00,\n",
      "68000,28.75,0.43,4.25,0.43,17.00,1.00,\n",
      "68250,29.00,0.00,4.50,0.50,17.00,1.00,\n",
      "68500,29.00,0.00,5.00,0.00,18.00,0.00,\n",
      "68750,28.67,0.47,4.00,0.00,16.67,0.94,\n",
      "69000,28.50,0.50,4.75,0.43,16.25,0.83,\n",
      "69250,28.75,0.43,5.00,0.00,16.50,0.50,\n",
      "69500,28.67,0.47,5.00,0.00,17.33,0.94,\n",
      "69750,28.67,0.47,5.00,0.00,17.00,0.71,\n",
      "70000,29.00,0.00,5.00,0.00,17.00,1.00,\n",
      "70250,29.00,0.00,4.25,0.43,17.00,1.00,\n",
      "70500,29.00,0.00,4.00,0.00,17.00,1.00,\n",
      "70750,29.00,0.00,5.00,0.00,17.33,0.94,\n",
      "71000,28.50,0.50,4.75,0.43,17.50,0.50,\n",
      "71250,29.00,0.00,4.67,0.47,17.50,0.50,\n",
      "71500,28.60,0.49,4.80,0.75,17.50,0.50,\n",
      "71750,28.50,0.50,5.00,0.00,17.50,0.50,\n",
      "72000,28.50,0.50,5.00,0.00,16.75,0.83,\n",
      "72250,28.75,0.43,8.67,0.47,15.67,1.25,\n",
      "72500,29.00,0.00,7.00,0.82,18.50,1.12,\n",
      "72750,30.00,0.00,11.50,1.50,17.50,1.50,\n",
      "73000,30.00,0.00,12.75,0.43,16.67,0.47,\n",
      "73250,30.00,0.00,12.00,0.00,17.75,1.09,\n",
      "73500,30.00,0.00,11.50,0.50,18.75,0.83,\n",
      "73750,30.00,0.00,11.00,0.00,19.50,1.12,\n",
      "74000,30.00,0.00,9.75,0.83,19.50,0.50,\n",
      "74250,30.00,0.00,9.00,0.00,18.33,0.47,\n",
      "74500,30.00,0.00,9.00,0.00,18.00,0.00,\n",
      "74750,30.00,0.00,8.50,0.50,18.00,0.00,\n",
      "75000,30.00,0.00,7.25,0.43,18.00,0.00,\n",
      "75250,30.00,0.00,7.25,0.43,18.50,0.50,\n",
      "75500,30.00,0.00,9.50,0.50,18.67,0.47,\n",
      "75750,30.00,0.00,11.00,0.00,18.50,0.50,\n",
      "76000,30.00,0.00,11.00,0.00,18.50,0.50,\n",
      "76250,30.00,0.00,10.75,0.43,18.50,0.50,\n",
      "76500,30.00,0.00,10.75,0.43,18.50,0.50,\n",
      "76750,30.00,0.00,10.75,0.43,18.75,0.83,\n",
      "77000,30.00,0.00,10.75,0.43,18.67,0.47,\n",
      "77250,30.00,0.00,10.50,0.50,18.67,0.47,\n",
      "77500,30.00,0.00,10.33,0.47,18.33,0.47,\n",
      "77750,30.00,0.00,10.00,0.00,18.67,0.47,\n",
      "78000,30.00,0.00,10.00,0.00,18.33,0.47,\n",
      "78250,30.00,0.00,9.50,0.50,18.00,0.00,\n",
      "78500,30.00,0.00,9.00,0.00,18.50,0.50,\n",
      "78750,30.00,0.00,8.00,0.00,18.50,0.50,\n",
      "79000,30.00,0.00,8.25,0.43,18.50,0.50,\n",
      "79250,30.00,0.00,8.00,0.00,18.67,0.47,\n",
      "79500,30.00,0.00,8.00,0.00,19.00,1.00,\n",
      "79750,30.00,0.00,8.00,0.00,19.00,1.00,\n",
      "80000,30.00,0.00,6.50,0.50,19.00,1.00,\n",
      "80250,30.00,0.00,7.25,0.43,19.67,0.47,\n",
      "80500,30.00,0.00,7.50,0.50,19.25,1.30,\n",
      "80750,30.00,0.00,7.33,0.47,20.00,0.82,\n",
      "81000,30.00,0.00,9.00,0.00,20.33,1.30,\n",
      "81250,30.00,0.00,7.50,1.66,19.25,0.83,\n",
      "81500,30.00,0.00,4.75,0.43,19.67,0.47,\n",
      "81750,29.75,0.43,7.75,1.30,19.00,1.00,\n",
      "82000,30.00,0.00,9.50,0.50,18.67,0.47,\n",
      "82250,30.00,0.00,11.60,0.49,18.75,0.83,\n",
      "82500,30.00,0.00,12.00,0.00,18.67,0.47,\n",
      "82750,29.75,0.43,12.00,0.00,18.25,0.43,\n",
      "83000,29.50,0.50,12.00,0.00,18.25,0.43,\n",
      "83250,29.50,0.50,12.00,0.00,18.25,0.43,\n",
      "83500,29.75,0.43,12.00,0.00,18.50,0.50,\n",
      "83750,29.75,0.43,12.00,0.00,18.25,0.43,\n",
      "84000,29.75,0.43,12.00,0.00,18.00,0.00,\n",
      "84250,29.50,0.50,12.00,0.00,18.00,0.00,\n",
      "84500,29.50,0.50,12.00,0.00,18.00,0.00,\n",
      "84750,29.00,0.00,12.00,0.00,18.00,0.00,\n",
      "85000,29.33,0.47,12.00,0.00,18.33,0.47,\n",
      "85250,29.00,0.00,12.00,0.00,18.33,0.47,\n",
      "85500,29.33,0.47,12.00,0.00,18.67,0.47,\n",
      "85750,29.33,0.47,12.00,0.00,18.50,0.50,\n",
      "86000,29.00,0.00,12.00,0.00,18.50,0.50,\n",
      "86250,29.25,0.43,12.00,0.00,18.50,0.50,\n",
      "86500,29.33,0.47,12.00,0.00,19.00,0.82,\n",
      "86750,29.25,0.43,12.00,0.00,18.33,0.47,\n",
      "87000,29.50,0.50,12.00,0.00,18.00,0.00,\n",
      "87250,29.00,0.00,12.00,0.00,18.33,0.47,\n",
      "87500,29.00,0.00,12.00,0.00,18.00,0.00,\n",
      "87750,30.00,0.00,12.00,0.00,19.00,0.00,\n",
      "88000,29.00,0.00,12.00,0.00,18.50,0.50,\n",
      "88250,29.33,0.47,12.00,0.00,18.67,0.47,\n",
      "88500,29.00,0.00,12.00,0.00,18.50,0.50,\n",
      "88750,28.33,0.47,12.00,0.00,19.00,0.82,\n",
      "89000,28.33,0.47,11.00,1.00,18.50,0.50,\n",
      "89250,28.25,0.43,7.50,0.50,18.25,0.43,\n",
      "89500,28.25,0.43,4.25,2.28,18.25,0.43,\n",
      "89750,29.00,0.00,11.00,1.22,17.50,0.50,\n",
      "90000,29.00,0.00,12.00,0.00,15.75,0.43,\n",
      "90250,29.67,0.47,12.00,0.00,16.67,1.25,\n",
      "90500,29.50,0.50,12.00,0.00,16.67,0.94,\n",
      "90750,29.00,0.00,12.33,0.47,17.50,0.50,\n",
      "91000,29.00,0.00,12.75,0.43,17.75,0.43,\n",
      "91250,29.50,0.50,12.75,0.43,18.00,0.00,\n",
      "91500,29.50,0.50,12.75,0.43,17.00,1.00,\n",
      "91750,29.00,0.00,12.25,0.43,17.33,0.94,\n",
      "92000,29.25,0.43,12.00,0.00,17.00,0.71,\n",
      "92250,29.00,0.00,12.00,0.00,17.00,0.82,\n",
      "92500,29.25,0.43,12.00,0.00,16.25,0.43,\n",
      "92750,29.00,0.00,11.67,0.47,16.67,1.25,\n",
      "93000,28.50,0.50,12.00,0.00,17.67,0.47,\n",
      "93250,29.00,0.00,12.50,0.50,18.33,0.47,\n",
      "93500,29.00,0.00,11.33,0.47,18.75,0.83,\n",
      "93750,29.00,0.00,12.00,0.00,19.33,0.94,\n",
      "94000,29.33,0.47,12.00,0.00,18.75,0.83,\n",
      "94250,29.33,0.47,12.00,0.00,19.00,0.82,\n",
      "94500,29.00,0.00,12.33,0.47,18.67,0.94,\n",
      "94750,29.00,0.00,12.75,0.43,19.33,0.94,\n",
      "95000,29.00,0.00,13.50,0.50,19.25,0.83,\n",
      "95250,29.00,0.00,13.00,0.00,19.25,0.83,\n",
      "95500,29.00,0.00,13.00,0.00,19.25,0.83,\n",
      "95750,28.50,0.50,13.50,0.50,19.00,1.00,\n",
      "96000,29.00,0.00,13.67,0.47,19.33,0.94,\n",
      "96250,28.67,0.47,12.00,0.71,18.33,0.47,\n",
      "96500,28.00,0.71,8.75,0.43,18.75,1.48,\n",
      "96750,28.50,0.50,9.00,0.00,19.25,0.83,\n",
      "97000,29.00,0.00,9.50,0.50,18.00,0.00,\n",
      "97250,28.50,0.50,8.50,0.50,18.25,0.43,\n",
      "97500,29.00,0.00,9.00,0.00,18.33,0.47,\n",
      "97750,29.00,0.00,9.50,0.50,19.00,0.71,\n",
      "98000,29.00,0.00,9.75,0.43,19.25,0.83,\n",
      "98250,28.67,0.47,10.00,0.00,19.67,0.47,\n",
      "98500,28.50,0.50,10.00,0.00,19.75,0.83,\n",
      "98750,29.00,0.00,10.00,0.00,20.33,0.94,\n",
      "99000,28.67,0.47,10.00,0.71,19.67,0.47,\n",
      "99250,29.00,0.00,10.33,0.47,19.00,1.00,\n",
      "99500,29.00,0.00,9.67,0.47,19.50,0.50,\n",
      "99750,29.00,0.00,10.25,0.43,19.00,0.82,\n",
      "100000,28.75,0.43,9.75,0.43,18.75,0.83,\n",
      "100250,28.50,0.50,9.75,0.43,19.25,0.83,\n",
      "100500,27.75,0.43,9.75,0.43,19.25,0.83,\n",
      "100750,28.00,0.00,10.00,0.00,19.33,0.94,\n",
      "101000,28.00,0.71,10.25,0.43,19.00,1.00,\n",
      "101250,28.00,0.00,11.00,0.00,19.25,0.83,\n",
      "101500,28.00,0.00,11.00,0.00,18.67,0.94,\n",
      "101750,28.00,0.00,11.00,0.00,19.25,0.83,\n",
      "102000,28.33,0.47,11.25,0.43,20.33,0.94,\n",
      "102250,28.00,0.00,12.00,0.00,19.00,0.71,\n",
      "102500,28.00,0.00,10.00,1.63,18.00,0.71,\n",
      "102750,28.00,0.00,7.25,1.09,19.50,1.12,\n",
      "103000,28.00,0.00,8.75,0.43,21.00,0.00,\n",
      "103250,28.25,0.43,9.25,0.43,20.25,0.83,\n",
      "103500,28.67,0.47,10.67,0.47,20.25,0.83,\n",
      "103750,29.00,0.00,12.00,0.00,20.67,0.47,\n",
      "104000,29.00,0.00,12.00,0.00,20.67,0.47,\n",
      "104250,28.75,0.43,11.50,0.50,19.75,0.83,\n",
      "104500,28.50,0.50,11.00,0.00,18.75,0.83,\n",
      "104750,28.75,0.43,11.25,0.43,18.50,0.50,\n",
      "105000,28.25,0.43,11.67,0.47,18.67,0.94,\n",
      "105250,28.50,0.50,12.00,0.00,19.25,0.83,\n",
      "105500,28.33,0.47,12.00,0.00,20.67,0.47,\n",
      "105750,28.00,0.00,12.00,0.00,20.25,0.83,\n",
      "106000,27.50,0.50,12.00,0.00,20.00,0.82,\n",
      "106250,28.00,0.00,12.00,0.00,21.00,0.00,\n",
      "106500,28.50,0.50,12.00,0.00,20.00,1.00,\n",
      "106750,28.00,0.00,12.00,0.00,19.33,0.94,\n",
      "107000,27.50,0.50,11.75,0.43,19.00,0.71,\n",
      "107250,28.00,0.00,11.33,0.47,18.75,0.83,\n",
      "107500,28.00,0.71,10.75,0.43,19.00,1.22,\n",
      "107750,27.50,0.87,11.25,0.83,20.50,0.87,\n",
      "108000,29.00,0.71,12.00,0.00,18.25,0.43,\n",
      "108250,30.00,0.00,12.00,0.00,18.00,0.00,\n",
      "108500,29.67,0.47,10.00,1.22,17.67,0.47,\n",
      "108750,29.00,0.00,10.00,0.00,18.33,1.25,\n",
      "109000,29.00,0.00,9.75,0.83,19.25,0.83,\n",
      "109250,28.75,0.43,11.00,1.22,20.00,1.00,\n",
      "109500,28.25,0.43,10.25,1.30,18.25,1.09,\n",
      "109750,29.00,0.00,12.00,0.00,17.33,0.94,\n",
      "110000,29.75,0.43,11.00,1.22,17.75,1.09,\n",
      "110250,30.00,0.00,9.33,0.47,18.75,0.83,\n",
      "110500,30.00,0.00,6.25,0.43,20.50,0.50,\n",
      "110750,30.00,0.00,8.25,0.83,19.00,1.00,\n",
      "111000,30.00,0.00,9.75,1.30,20.25,0.83,\n",
      "111250,29.75,0.43,9.00,0.00,19.67,0.94,\n",
      "111500,30.00,0.00,8.75,0.43,20.00,0.71,\n",
      "111750,29.33,0.47,9.00,1.58,20.00,0.82,\n",
      "112000,29.25,0.43,10.75,0.43,18.75,0.83,\n",
      "112250,29.25,0.43,11.50,0.50,18.50,0.50,\n",
      "112500,29.33,0.47,9.00,0.00,18.50,0.50,\n",
      "112750,29.00,0.00,10.25,0.43,18.25,0.43,\n",
      "113000,28.50,0.50,10.00,0.71,18.50,0.50,\n",
      "113250,28.50,0.50,9.00,0.00,18.50,0.50,\n",
      "113500,28.75,0.43,9.25,0.43,18.50,0.50,\n",
      "113750,28.75,0.43,8.25,0.83,17.75,0.43,\n",
      "114000,29.00,0.00,9.50,0.87,17.50,0.87,\n",
      "114250,28.50,0.50,9.33,0.94,16.33,0.47,\n",
      "114500,28.50,0.50,11.75,0.43,20.00,0.71,\n",
      "114750,28.25,0.43,10.50,0.50,20.50,0.50,\n",
      "115000,28.50,0.50,9.25,0.43,20.25,0.83,\n",
      "115250,28.33,0.47,11.75,0.43,20.33,0.94,\n",
      "115500,28.67,0.47,10.00,0.82,20.25,0.83,\n",
      "115750,28.67,0.47,9.00,0.00,21.00,0.00,\n",
      "116000,28.50,0.50,8.75,0.83,21.00,0.00,\n",
      "116250,29.00,0.00,7.50,0.50,20.50,0.50,\n",
      "116500,29.25,0.43,6.75,0.43,20.50,0.50,\n",
      "116750,29.00,0.00,7.67,0.47,20.75,0.43,\n",
      "117000,28.25,0.43,9.00,0.71,20.75,0.43,\n",
      "117250,29.00,0.00,11.25,0.43,21.00,0.00,\n",
      "117500,29.00,0.00,12.00,0.00,20.75,0.43,\n",
      "117750,29.00,0.71,11.25,0.43,21.25,0.43,\n",
      "118000,29.00,0.00,10.00,0.82,21.33,0.47,\n",
      "118250,28.67,0.47,11.00,1.00,19.33,1.25,\n",
      "118500,28.50,0.50,2.33,1.25,16.60,1.20,\n",
      "118750,28.67,0.47,4.67,1.25,17.33,0.47,\n",
      "119000,27.50,0.50,5.50,2.50,17.25,1.30,\n",
      "119250,28.00,0.00,6.67,0.94,17.00,1.00,\n",
      "119500,28.00,0.00,5.00,0.82,17.00,0.71,\n",
      "119750,28.00,0.00,0.00,0.00,17.00,1.00,\n"
     ]
    }
   ],
   "source": [
    "logi_regr.prepare_datset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b26ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_regr.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d979bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_regr.labelencode_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be4d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8ab431ac8f4ebebcf6119c9c1f5946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp_df = logi_regr.df\n",
    "logi_regr.create_profile_report(inp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_regr.handle_outlier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc426af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_regr.standard_scaling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_regr.vif_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee506e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_regr.drop_multicolinearity_by_vif(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d0ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_regr.train_test_split(0.2,144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cdfc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the models to evaluate\n",
    "models = logi_regr.get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_dict, roc_auc_score_dict, precision_score_dict, recall_score_dict, f1_score_dict = logi_regr.fit_evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0fc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, accuracy in accuracy_score_dict.items():\n",
    "    hyper_param = name.split('_')\n",
    "    print('\\nc={}, penalty={}, solver={}-->Accuracy: {:.3f}\\n'.format(hyper_param[0], hyper_param[1], hyper_param[2], accuracy))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b051f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, roc_auc in roc_auc_score_dict.items():\n",
    "    hyper_param = name.split('_')\n",
    "    print('\\nc={}, penalty={}, solver={}-->roc_auc_score: {:.3f}\\n'.format(hyper_param[0], hyper_param[1], hyper_param[2], roc_auc))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, precision in precision_score_dict.items():\n",
    "    hyper_param = name.split('_')\n",
    "    print('\\nc={}, penalty={}, solver={}-->precision_score: {:.3f}\\n'.format(hyper_param[0], hyper_param[1], hyper_param[2], precision))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d459c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, recall in recall_score_dict.items():\n",
    "    hyper_param = name.split('_')\n",
    "    print('\\nc={}, penalty={}, solver={}-->recall_score: {:.3f}\\n'.format(hyper_param[0], hyper_param[1], hyper_param[2], recall))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c18c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, f1 in f1_score_dict.items():\n",
    "    hyper_param = name.split('_')\n",
    "    print('\\nc={}, penalty={}, solver={}-->f1_score: {:.3f}\\n'.format(hyper_param[0], hyper_param[1], hyper_param[2], f1))                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a78a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    # evaluate the model and collect the scores\n",
    "    scores = logi_regr.evaluate_model_using_cv(model, logi_regr.x_scaled , logi_regr.y)\n",
    "    # store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    # summarize progress along the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.gcf().autofmt_xdate()\n",
    "pyplot.rcParams[\"figure.figsize\"] = (10,10)\n",
    "pyplot.tight_layout()\n",
    "pyplot.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
